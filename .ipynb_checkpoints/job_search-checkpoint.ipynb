{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests , json , csv , time , re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from google.cloud import translate\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "subscription_key = 'e66fc219be674ca0839b810acea5a4cc'\n",
    "endpoint = 'https://saym-text.cognitiveservices.azure.com/'\n",
    "client = translate.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jobs = ['werkstudent','praktikant']\n",
    "Languages = ['azure','angular','react','python','java','c++','javascript',\n",
    "             'deep learning','machine learning','artificial intelligence',\n",
    "             'data analysis','data science','matlab','django','front-end','back-end',\n",
    "            'frontend','backend','front end','back end','linux','webpack','mysql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticateClient():\n",
    "    credentials = CognitiveServicesCredentials(subscription_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint, credentials=credentials)\n",
    "    return text_analytics_client\n",
    "\n",
    "def entity_recognition(Text):\n",
    "    entityData = set()\n",
    "    \n",
    "    client = authenticateClient()\n",
    "\n",
    "    try:\n",
    "        documents = [\n",
    "            {\"id\": \"1\", \"language\": \"en\", \"text\": Text}\n",
    "        ]\n",
    "        response = client.entities(documents=documents)\n",
    "\n",
    "        for document in response.documents:\n",
    "            for entity in document.entities:\n",
    "                entityData.add(entity.name)\n",
    "    \n",
    "        response = client.key_phrases(documents=documents)\n",
    "\n",
    "        for document in response.documents:\n",
    "            for phrase in document.key_phrases:\n",
    "                entityData.add(phrase)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "    return entityData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_search_details = pd.DataFrame(columns={'Title', 'Url','Place', 'Time'})\n",
    "for p in Jobs:\n",
    "    Url = 'https://www.stepstone.de/5/job-search-simple.html?stf=freeText&ns=1&ke='+p+'&ws=essen&ra=50&li=100&wt=80002'\n",
    "    Request = requests.get(Url,'lxml')\n",
    "    HTML = bs(Request.content)\n",
    "    Links = HTML.find_all('article')\n",
    "    for i in range(len(Links)):\n",
    "        job_title = Links[i].h2.text\n",
    "        job_links = Links[i].find_all('a')\n",
    "        job_url = 'https://www.stepstone.de'+job_links[1]['href']\n",
    "        job_place_time = Links[i].find_all('li')\n",
    "        job_place = job_place_time[1].text\n",
    "        job_time = job_place_time[0].text\n",
    "        Job_search_details = Job_search_details.append({\n",
    "            'Title': job_title,\n",
    "            'Url' : job_url,\n",
    "            'Place' : job_place,\n",
    "            'Time' : job_time\n",
    "        }, ignore_index = True)\n",
    "#         print(job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT working student in the field of digitization (m / f / d) up to 20 hours per week\n",
      "Working student (m / f / d) digitization\n",
      "Intern / Working student digital transformation (f / m / d)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Temp_jobs = pd.DataFrame()\n",
    "Full_Job_Des = pd.DataFrame()\n",
    "\n",
    "for index , row in Job_search_details.iterrows():\n",
    "    Job_req_des = dict()\n",
    "    Title = row['Title']\n",
    "    Place = row['Place']\n",
    "    Time = row['Time']\n",
    "    Links = row['Url']\n",
    "    Request = requests.get(Links)\n",
    "    Pages = bs(Request.content,'lxml')\n",
    "    Job_Title = Pages.find('h1').text\n",
    "    Main = Pages.find_all('main')\n",
    "    if Main != None or Main != '':\n",
    "        try:\n",
    "            str_list = Main[1].text.split('\\n')\n",
    "        except:\n",
    "            str_list = '[]'\n",
    "            \n",
    "        Texts = [x.replace('\\xa0',' ') for x in str_list if x != '' and x !='\\xa0' and x != ' ']\n",
    "        NameSet = set()\n",
    "        for Lines in Texts:\n",
    "            for name in Languages:\n",
    "                if Lines.lower().find(name)!=-1:\n",
    "                    NameSet.add(name)\n",
    "                    \n",
    "        if len(NameTemp) !=0:\n",
    "            En_texts = list()\n",
    "            EntitySet = set()               \n",
    "            Texts_en = client.translate(Texts)\n",
    "            Title_en = client.translate(Job_Title)\n",
    "            Title_en = Title_en['translatedText']\n",
    "            \n",
    "            for Eng in Texts_en:\n",
    "                EngText = Eng['translatedText']\n",
    "                Entites = entity_recognition(EngText)\n",
    "                for i in Entites:\n",
    "                    EntitySet.add(i)\n",
    "                    En_texts.append(EngText)\n",
    "\n",
    "\n",
    "           \n",
    "            for Entity in EntitySet:\n",
    "                for name in Languages:\n",
    "                    if Entity.lower().find(name)!=-1:\n",
    "                        NameSet.add(name)\n",
    "\n",
    "            Full_Job_Des = Full_Job_Des.append({\n",
    "                    'Job Title': Job_Title,\n",
    "                    'Job Title En':Title_en,\n",
    "                    'Job Url' : Links,\n",
    "                    'Place' : Place,\n",
    "                    'Time' : Time,\n",
    "                    'Job Details': Texts,\n",
    "                    'Job Details English':En_texts,\n",
    "                    'Entities': EntitySet,\n",
    "                    'Important Points': NameSet\n",
    "                },ignore_index = True)\n",
    "            print(Title_en)\n",
    "Full_Job_Des.to_json('Full_Details.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_Job_Des = pd.DataFrame()\n",
    "# for index , row in Temp_jobs.iterrows():\n",
    "#     En_texts = list()\n",
    "#     EntitySet = set()        \n",
    "#     Texts = row['Job Details']\n",
    "#     Job_Title = row['Job Title']        \n",
    "#     Texts_en = client.translate(Texts)\n",
    "#     Title_en = client.translate(Job_Title)\n",
    "# #     for Tit_en in Title_en:\n",
    "#     Title_en = Title_en['translatedText']\n",
    "    \n",
    "#     for Eng in Texts_en:\n",
    "#         EngText = Eng['translatedText']\n",
    "#         Entites = entity_recognition(EngText)\n",
    "#         for i in Entites:\n",
    "#             EntitySet.add(i)\n",
    "#             En_texts.append(EngText)\n",
    "       \n",
    "        \n",
    "#     NameSet = set()\n",
    "#     for Entity in EntitySet:\n",
    "#         for name in Languages:\n",
    "#             if Entity.lower().find(name)!=-1:\n",
    "#                 NameSet.add(name)\n",
    "                    \n",
    "#     Full_Job_Des = Full_Job_Des.append({\n",
    "#             'Job Title': Job_Title,\n",
    "#             'Job Title En':Title_en,\n",
    "#             'Job Url' : row['Job Url'],\n",
    "#             'Place' : row['Place'],\n",
    "#             'Time' : row['Time'],\n",
    "#             'Job Details': Texts,\n",
    "#             'Job Details English':En_texts,\n",
    "#             'Entities': EntitySet,\n",
    "#             'Important Points': NameSet\n",
    "#         },ignore_index = True)\n",
    "#     print(Title_en)\n",
    "    \n",
    "# Full_Job_Des.to_json('Full_Details.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Details = pd.read_json('Full_Details.json')\n",
    "# for i in range(len(Details)):\n",
    "\n",
    "#     Entities = Details['Entities'][i]\n",
    "#     for j in Entities:\n",
    "#         for name in Languages:\n",
    "#             if j.lower().find(name.lower())!= -1:\n",
    "#                 print(Details['Job Title'][i],name)\n",
    "# #                 print(Entities)\n",
    "# #                 print(Details['Job Details English'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
